{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108747be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/mousumi/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb013aabec0403cbd0718266d070147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SQuAD 2.0 dataset\n",
    "dataset = load_dataset(\"squad_v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0a0242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f23566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Load the dataset (assuming you already have it loaded as `dataset`)\n",
    "# dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# def extract_start_positions(batch):\n",
    "#     # Initialize the list for start positions\n",
    "#     start_positions = []\n",
    "\n",
    "#     # Process each answer in the batch\n",
    "#     for answers in batch['answers']:\n",
    "#         # Check if 'answer_start' exists and is not empty\n",
    "#         if 'answer_start' in answers and len(answers['answer_start']) > 0:\n",
    "#             start_positions.append(answers['answer_start'][0])\n",
    "#         else:\n",
    "#             # Append a default value or handle the case where no start position is available\n",
    "#             start_positions.append(None)  # or use a specific marker like -1\n",
    "\n",
    "#     # Return the updated batch with new start_positions key\n",
    "#     return {'start_positions': start_positions}\n",
    "\n",
    "# # Apply the function to the dataset\n",
    "# tokenized_datasets = dataset.map(extract_start_positions, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# # Example of accessing the modified dataset\n",
    "# print(tokenized_datasets['train']['start_positions'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd396f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4839b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9552cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdacf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c939d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "#check for gpu\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b908155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.has_mps else \"cpu\"\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f1de98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f238b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Time:  0.16701412200927734\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# syncrocnize time with cpu, otherwise only time for oflaoding data to gpu would be measured\n",
    "#torch.mps.synchronize()\n",
    "\n",
    "a = torch.ones(4000,4000, device=\"mps\")\n",
    "for _ in range(200):\n",
    "    a +=a\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print( \"GPU Time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64c2b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Setup device\n",
    "device = \"mps\" if torch.has_mps else \"cpu\"\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to the appropriate device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "412dc808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4f545b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_squad_examples(examples):\n",
    "    # Encode the inputs using batched=True to handle large datasets efficiently\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        max_length=384,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset_mapping in enumerate(tokenized_inputs[\"offset_mapping\"]):\n",
    "        answer = examples['answers'][i]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        answer_length = len(answer['text'][0])\n",
    "        \n",
    "        sequence_ids = tokenized_inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start index\n",
    "        start_index = 0\n",
    "        while sequence_ids[start_index] != 1:\n",
    "            start_index += 1\n",
    "        while start_index < len(offset_mapping) and (offset_mapping[start_index][0] <= start_char and offset_mapping[start_index][1] <= start_char):\n",
    "            start_index += 1\n",
    "        \n",
    "        # Ensure start_index is within range\n",
    "        if start_index >= len(offset_mapping):\n",
    "            start_index = len(offset_mapping) - 1\n",
    "\n",
    "        # Find the end index based on the length of the answer\n",
    "        end_index = start_index\n",
    "        while end_index < len(offset_mapping) and (offset_mapping[end_index][1] - offset_mapping[start_index][0] < answer_length):\n",
    "            end_index += 1\n",
    "\n",
    "        # Ensure end_index is within range\n",
    "        end_index = min(end_index, len(offset_mapping) - 1)\n",
    "\n",
    "        start_positions.append(start_index)\n",
    "        end_positions.append(end_index)\n",
    "\n",
    "    tokenized_inputs[\"start_positions\"] = start_positions\n",
    "    tokenized_inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    # Remove the offset mapping to avoid saving it during training\n",
    "    del tokenized_inputs[\"offset_mapping\"]\n",
    "    \n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01994333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/mousumi/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fc4e82e4864c57b3f1ee6781e362e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/mousumi/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cc819505d5d56daf.arrow\n",
      "Loading cached processed dataset at /Users/mousumi/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b6d27f9939969ad7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 68, 143]\n",
      "[77, 69, 142]\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_and_align_labels(batch):\n",
    "    # Tokenize the text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['question'],\n",
    "        batch['context'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=384,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Iterate over each example in the batch\n",
    "    for i in range(len(batch['question'])):\n",
    "        # Check if 'answer_start' and 'text' are available\n",
    "        if 'answer_start' in batch['answers'][i] and len(batch['answers'][i]['answer_start']) > 0:\n",
    "            start_char = batch['answers'][i]['answer_start'][0]\n",
    "            answer_text = batch['answers'][i]['text'][0]\n",
    "        else:\n",
    "            # Default values if data is missing\n",
    "            start_char = 0\n",
    "            answer_text = \"\"\n",
    "\n",
    "        # Find the position of the answer within the tokenized context\n",
    "        offset_mapping = tokenized_inputs['offset_mapping'][i]\n",
    "\n",
    "        # Attempt to find the start token index\n",
    "        start_token_index = next((idx for idx, offset in enumerate(offset_mapping) if start_char >= offset[0] and start_char < offset[1]), None)\n",
    "\n",
    "        # Find the end token index based on the length of the answer\n",
    "        end_token_index = start_token_index + len(tokenizer.encode(answer_text, add_special_tokens=False)) - 2 if start_token_index is not None else None\n",
    "\n",
    "        start_positions.append(start_token_index if start_token_index is not None else 0)\n",
    "        end_positions.append(end_token_index if end_token_index is not None else 0)\n",
    "\n",
    "    # Save these positions back into the tokenized_inputs\n",
    "    tokenized_inputs['start_positions'] = start_positions\n",
    "    tokenized_inputs['end_positions'] = end_positions\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Load the dataset and apply the tokenization\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Optional: show a preview\n",
    "print(tokenized_datasets['train']['start_positions'][:3])\n",
    "print(tokenized_datasets['train']['end_positions'][:3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad01d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 16290\n",
      "Number of validation batches: 1485\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Function to convert dataset features to tensors\n",
    "def convert_to_tensors(dataset):\n",
    "    # Convert lists of examples into tensors for each feature\n",
    "    all_input_ids = torch.tensor(dataset['input_ids'], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(dataset['attention_mask'], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor(dataset['token_type_ids'], dtype=torch.long)\n",
    "    all_start_positions = torch.tensor(dataset['start_positions'], dtype=torch.long)\n",
    "    all_end_positions = torch.tensor(dataset['end_positions'], dtype=torch.long)\n",
    "\n",
    "    return TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_start_positions, all_end_positions)\n",
    "\n",
    "# Assuming 'tokenized_datasets' is a DatasetDict containing processed data\n",
    "# Load your dataset (only necessary if not already loaded and processed)\n",
    "# dataset = load_dataset(\"squad_v2\")\n",
    "# tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Convert the processed dataset into tensors\n",
    "train_dataset = convert_to_tensors(tokenized_datasets['train'])\n",
    "validation_dataset = convert_to_tensors(tokenized_datasets['validation'])\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8)\n",
    "\n",
    "# Optionally print some information to confirm setup\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(validation_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/mousumi/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.897670269012451\n",
      "Epoch 1, Loss: 5.883021354675293\n",
      "Epoch 1, Loss: 5.9113054275512695\n",
      "Epoch 1, Loss: 5.867959976196289\n",
      "Epoch 1, Loss: 5.958220481872559\n",
      "Epoch 1, Loss: 5.816848278045654\n",
      "Epoch 1, Loss: 5.670872688293457\n",
      "Epoch 1, Loss: 5.788008689880371\n",
      "Epoch 1, Loss: 5.991209983825684\n",
      "Epoch 1, Loss: 5.904509544372559\n",
      "Epoch 1, Loss: 5.886578559875488\n",
      "Epoch 1, Loss: 5.957003116607666\n",
      "Epoch 1, Loss: 5.867313385009766\n",
      "Epoch 1, Loss: 5.788227081298828\n",
      "Epoch 1, Loss: 5.830669403076172\n",
      "Epoch 1, Loss: 5.775439262390137\n",
      "Epoch 1, Loss: 5.663948059082031\n",
      "Epoch 1, Loss: 5.913341045379639\n",
      "Epoch 1, Loss: 5.813055515289307\n",
      "Epoch 1, Loss: 5.7201714515686035\n",
      "Epoch 1, Loss: 5.839244842529297\n",
      "Epoch 1, Loss: 5.769522666931152\n",
      "Epoch 1, Loss: 5.927454471588135\n",
      "Epoch 1, Loss: 5.761710166931152\n",
      "Epoch 1, Loss: 5.806033611297607\n",
      "Epoch 1, Loss: 5.905063629150391\n",
      "Epoch 1, Loss: 5.836753845214844\n",
      "Epoch 1, Loss: 5.794981002807617\n",
      "Epoch 1, Loss: 5.871476173400879\n",
      "Epoch 1, Loss: 5.837308883666992\n",
      "Epoch 1, Loss: 5.843579292297363\n",
      "Epoch 1, Loss: 5.843292236328125\n",
      "Epoch 1, Loss: 5.7636613845825195\n",
      "Epoch 1, Loss: 5.7841949462890625\n",
      "Epoch 1, Loss: 5.659340858459473\n",
      "Epoch 1, Loss: 5.769617080688477\n",
      "Epoch 1, Loss: 5.886196136474609\n",
      "Epoch 1, Loss: 5.799598693847656\n",
      "Epoch 1, Loss: 5.8478546142578125\n",
      "Epoch 1, Loss: 5.867236137390137\n",
      "Epoch 1, Loss: 5.7575225830078125\n",
      "Epoch 1, Loss: 5.958743572235107\n",
      "Epoch 1, Loss: 5.818181037902832\n",
      "Epoch 1, Loss: 5.801205635070801\n",
      "Epoch 1, Loss: 5.955733776092529\n",
      "Epoch 1, Loss: 5.888325214385986\n",
      "Epoch 1, Loss: 5.888960838317871\n",
      "Epoch 1, Loss: 5.6698808670043945\n",
      "Epoch 1, Loss: 5.762691497802734\n",
      "Epoch 1, Loss: 5.723794460296631\n",
      "Epoch 1, Loss: 5.967805862426758\n",
      "Epoch 1, Loss: 5.697583198547363\n",
      "Epoch 1, Loss: 5.719046592712402\n",
      "Epoch 1, Loss: 5.6046881675720215\n",
      "Epoch 1, Loss: 5.878382682800293\n",
      "Epoch 1, Loss: 5.663442134857178\n",
      "Epoch 1, Loss: 5.939262390136719\n",
      "Epoch 1, Loss: 5.6717634201049805\n",
      "Epoch 1, Loss: 5.890650749206543\n",
      "Epoch 1, Loss: 5.846949100494385\n",
      "Epoch 1, Loss: 5.718596458435059\n",
      "Epoch 1, Loss: 5.744731903076172\n",
      "Epoch 1, Loss: 5.826602935791016\n",
      "Epoch 1, Loss: 5.942594528198242\n",
      "Epoch 1, Loss: 5.6914381980896\n",
      "Epoch 1, Loss: 5.6869306564331055\n",
      "Epoch 1, Loss: 5.6850080490112305\n",
      "Epoch 1, Loss: 5.642278671264648\n",
      "Epoch 1, Loss: 5.7347307205200195\n",
      "Epoch 1, Loss: 5.7843732833862305\n",
      "Epoch 1, Loss: 5.7036590576171875\n",
      "Epoch 1, Loss: 5.73317289352417\n",
      "Epoch 1, Loss: 5.566767692565918\n",
      "Epoch 1, Loss: 5.915820121765137\n",
      "Epoch 1, Loss: 5.698272228240967\n",
      "Epoch 1, Loss: 5.731208801269531\n",
      "Epoch 1, Loss: 5.743695259094238\n",
      "Epoch 1, Loss: 5.618910789489746\n",
      "Epoch 1, Loss: 5.658560276031494\n",
      "Epoch 1, Loss: 5.688030242919922\n",
      "Epoch 1, Loss: 5.735068321228027\n",
      "Epoch 1, Loss: 5.654231071472168\n",
      "Epoch 1, Loss: 5.694572448730469\n",
      "Epoch 1, Loss: 5.79475736618042\n",
      "Epoch 1, Loss: 5.600763320922852\n",
      "Epoch 1, Loss: 5.706239223480225\n",
      "Epoch 1, Loss: 5.674928665161133\n",
      "Epoch 1, Loss: 5.729887962341309\n",
      "Epoch 1, Loss: 5.662598133087158\n",
      "Epoch 1, Loss: 5.738191604614258\n",
      "Epoch 1, Loss: 5.49674129486084\n",
      "Epoch 1, Loss: 5.85801887512207\n",
      "Epoch 1, Loss: 5.625156879425049\n",
      "Epoch 1, Loss: 5.723812103271484\n",
      "Epoch 1, Loss: 5.730704307556152\n",
      "Epoch 1, Loss: 5.749062538146973\n",
      "Epoch 1, Loss: 5.838505268096924\n",
      "Epoch 1, Loss: 5.604312896728516\n",
      "Epoch 1, Loss: 5.528426170349121\n",
      "Epoch 1, Loss: 5.5491766929626465\n",
      "Epoch 1, Loss: 5.5512237548828125\n",
      "Epoch 1, Loss: 5.706079483032227\n",
      "Epoch 1, Loss: 5.520990371704102\n",
      "Epoch 1, Loss: 5.665660858154297\n",
      "Epoch 1, Loss: 5.288674354553223\n",
      "Epoch 1, Loss: 5.462773323059082\n",
      "Epoch 1, Loss: 5.461430072784424\n",
      "Epoch 1, Loss: 5.7364912033081055\n",
      "Epoch 1, Loss: 5.6392340660095215\n",
      "Epoch 1, Loss: 5.4988112449646\n",
      "Epoch 1, Loss: 5.747885704040527\n",
      "Epoch 1, Loss: 5.726901531219482\n",
      "Epoch 1, Loss: 5.581707000732422\n",
      "Epoch 1, Loss: 5.433072090148926\n",
      "Epoch 1, Loss: 5.749068260192871\n",
      "Epoch 1, Loss: 5.755631923675537\n",
      "Epoch 1, Loss: 5.547019004821777\n",
      "Epoch 1, Loss: 5.761404037475586\n",
      "Epoch 1, Loss: 5.565046310424805\n",
      "Epoch 1, Loss: 5.644062042236328\n",
      "Epoch 1, Loss: 5.449342727661133\n",
      "Epoch 1, Loss: 5.532169342041016\n",
      "Epoch 1, Loss: 5.3767218589782715\n",
      "Epoch 1, Loss: 5.533311367034912\n",
      "Epoch 1, Loss: 5.6654205322265625\n",
      "Epoch 1, Loss: 5.30940055847168\n",
      "Epoch 1, Loss: 5.374834060668945\n",
      "Epoch 1, Loss: 5.321404457092285\n",
      "Epoch 1, Loss: 5.533419609069824\n",
      "Epoch 1, Loss: 5.349395751953125\n",
      "Epoch 1, Loss: 5.445418357849121\n",
      "Epoch 1, Loss: 5.303782939910889\n",
      "Epoch 1, Loss: 5.495389938354492\n",
      "Epoch 1, Loss: 5.16782283782959\n",
      "Epoch 1, Loss: 5.5726518630981445\n",
      "Epoch 1, Loss: 5.463920593261719\n",
      "Epoch 1, Loss: 5.17107629776001\n",
      "Epoch 1, Loss: 5.178701400756836\n",
      "Epoch 1, Loss: 5.561639785766602\n",
      "Epoch 1, Loss: 5.0192155838012695\n",
      "Epoch 1, Loss: 5.092886924743652\n",
      "Epoch 1, Loss: 5.541049957275391\n",
      "Epoch 1, Loss: 5.5034942626953125\n",
      "Epoch 1, Loss: 5.121048927307129\n",
      "Epoch 1, Loss: 4.948365211486816\n",
      "Epoch 1, Loss: 5.436328411102295\n",
      "Epoch 1, Loss: 5.414301872253418\n",
      "Epoch 1, Loss: 5.463250637054443\n",
      "Epoch 1, Loss: 5.428430557250977\n",
      "Epoch 1, Loss: 5.170316696166992\n",
      "Epoch 1, Loss: 5.3239030838012695\n",
      "Epoch 1, Loss: 5.293551445007324\n",
      "Epoch 1, Loss: 5.264389514923096\n",
      "Epoch 1, Loss: 5.566685676574707\n",
      "Epoch 1, Loss: 5.111095428466797\n",
      "Epoch 1, Loss: 5.207545280456543\n",
      "Epoch 1, Loss: 4.84432315826416\n",
      "Epoch 1, Loss: 5.145390510559082\n",
      "Epoch 1, Loss: 4.646988391876221\n",
      "Epoch 1, Loss: 5.04836368560791\n",
      "Epoch 1, Loss: 4.846963405609131\n",
      "Epoch 1, Loss: 5.194912910461426\n",
      "Epoch 1, Loss: 4.8607282638549805\n",
      "Epoch 1, Loss: 5.048912525177002\n",
      "Epoch 1, Loss: 4.618695259094238\n",
      "Epoch 1, Loss: 5.588718414306641\n",
      "Epoch 1, Loss: 5.178906440734863\n",
      "Epoch 1, Loss: 5.184847831726074\n",
      "Epoch 1, Loss: 4.755952835083008\n",
      "Epoch 1, Loss: 4.677288055419922\n",
      "Epoch 1, Loss: 5.3075103759765625\n",
      "Epoch 1, Loss: 5.478374481201172\n",
      "Epoch 1, Loss: 5.048154354095459\n",
      "Epoch 1, Loss: 4.963271617889404\n",
      "Epoch 1, Loss: 5.148075103759766\n",
      "Epoch 1, Loss: 4.948473930358887\n",
      "Epoch 1, Loss: 4.787956714630127\n",
      "Epoch 1, Loss: 4.927119731903076\n",
      "Epoch 1, Loss: 5.517050266265869\n",
      "Epoch 1, Loss: 4.9240312576293945\n",
      "Epoch 1, Loss: 4.745144367218018\n",
      "Epoch 1, Loss: 4.697211742401123\n",
      "Epoch 1, Loss: 5.0127410888671875\n",
      "Epoch 1, Loss: 4.915835380554199\n",
      "Epoch 1, Loss: 4.76657772064209\n",
      "Epoch 1, Loss: 4.836531639099121\n",
      "Epoch 1, Loss: 4.587342739105225\n",
      "Epoch 1, Loss: 4.200408458709717\n",
      "Epoch 1, Loss: 4.657614707946777\n",
      "Epoch 1, Loss: 4.674665451049805\n",
      "Epoch 1, Loss: 5.216820240020752\n",
      "Epoch 1, Loss: 4.616676330566406\n",
      "Epoch 1, Loss: 5.158382415771484\n",
      "Epoch 1, Loss: 4.8269124031066895\n",
      "Epoch 1, Loss: 4.161924839019775\n",
      "Epoch 1, Loss: 4.305757522583008\n",
      "Epoch 1, Loss: 4.835841655731201\n",
      "Epoch 1, Loss: 5.176870346069336\n",
      "Epoch 1, Loss: 4.371210098266602\n",
      "Epoch 1, Loss: 4.670101642608643\n",
      "Epoch 1, Loss: 5.0576934814453125\n",
      "Epoch 1, Loss: 4.7173357009887695\n",
      "Epoch 1, Loss: 4.461737632751465\n",
      "Epoch 1, Loss: 4.676875591278076\n",
      "Epoch 1, Loss: 4.429485321044922\n",
      "Epoch 1, Loss: 4.601994514465332\n",
      "Epoch 1, Loss: 4.593250274658203\n",
      "Epoch 1, Loss: 4.113016128540039\n",
      "Epoch 1, Loss: 4.274479389190674\n",
      "Epoch 1, Loss: 5.152505874633789\n",
      "Epoch 1, Loss: 3.950779438018799\n",
      "Epoch 1, Loss: 4.059378623962402\n",
      "Epoch 1, Loss: 4.85942268371582\n",
      "Epoch 1, Loss: 4.018836498260498\n",
      "Epoch 1, Loss: 3.919152021408081\n",
      "Epoch 1, Loss: 4.542438983917236\n",
      "Epoch 1, Loss: 4.5679731369018555\n",
      "Epoch 1, Loss: 3.9909591674804688\n",
      "Epoch 1, Loss: 4.0501251220703125\n",
      "Epoch 1, Loss: 4.4095282554626465\n",
      "Epoch 1, Loss: 4.178441524505615\n",
      "Epoch 1, Loss: 4.035441875457764\n",
      "Epoch 1, Loss: 4.3173441886901855\n",
      "Epoch 1, Loss: 3.843313217163086\n",
      "Epoch 1, Loss: 4.032805442810059\n",
      "Epoch 1, Loss: 4.335475921630859\n",
      "Epoch 1, Loss: 4.16375732421875\n",
      "Epoch 1, Loss: 3.5796103477478027\n",
      "Epoch 1, Loss: 4.05389928817749\n",
      "Epoch 1, Loss: 3.740647077560425\n",
      "Epoch 1, Loss: 4.176675796508789\n",
      "Epoch 1, Loss: 3.709524631500244\n",
      "Epoch 1, Loss: 4.199481964111328\n",
      "Epoch 1, Loss: 3.6978039741516113\n",
      "Epoch 1, Loss: 4.442899227142334\n",
      "Epoch 1, Loss: 3.990914821624756\n",
      "Epoch 1, Loss: 3.913637638092041\n",
      "Epoch 1, Loss: 3.9192147254943848\n",
      "Epoch 1, Loss: 4.879137992858887\n",
      "Epoch 1, Loss: 3.9976119995117188\n",
      "Epoch 1, Loss: 4.211034774780273\n",
      "Epoch 1, Loss: 4.018285751342773\n",
      "Epoch 1, Loss: 3.525125026702881\n",
      "Epoch 1, Loss: 3.555899143218994\n",
      "Epoch 1, Loss: 4.397540092468262\n",
      "Epoch 1, Loss: 3.902773857116699\n",
      "Epoch 1, Loss: 4.7671661376953125\n",
      "Epoch 1, Loss: 4.00546407699585\n",
      "Epoch 1, Loss: 4.431267738342285\n",
      "Epoch 1, Loss: 3.397639036178589\n",
      "Epoch 1, Loss: 3.9560561180114746\n",
      "Epoch 1, Loss: 4.868540287017822\n",
      "Epoch 1, Loss: 2.77921724319458\n",
      "Epoch 1, Loss: 3.6299993991851807\n",
      "Epoch 1, Loss: 3.1426920890808105\n",
      "Epoch 1, Loss: 3.096846580505371\n",
      "Epoch 1, Loss: 4.6376142501831055\n",
      "Epoch 1, Loss: 3.9215283393859863\n",
      "Epoch 1, Loss: 4.704111099243164\n",
      "Epoch 1, Loss: 3.7777442932128906\n",
      "Epoch 1, Loss: 4.545412063598633\n",
      "Epoch 1, Loss: 3.8302559852600098\n",
      "Epoch 1, Loss: 3.3281843662261963\n",
      "Epoch 1, Loss: 3.8961021900177\n",
      "Epoch 1, Loss: 2.7716925144195557\n",
      "Epoch 1, Loss: 3.718538999557495\n",
      "Epoch 1, Loss: 3.89725399017334\n",
      "Epoch 1, Loss: 3.86722469329834\n",
      "Epoch 1, Loss: 4.359023094177246\n",
      "Epoch 1, Loss: 3.623292922973633\n",
      "Epoch 1, Loss: 4.739985942840576\n",
      "Epoch 1, Loss: 4.092779636383057\n",
      "Epoch 1, Loss: 4.429100513458252\n",
      "Epoch 1, Loss: 4.233596324920654\n",
      "Epoch 1, Loss: 4.247834205627441\n",
      "Epoch 1, Loss: 3.9226837158203125\n",
      "Epoch 1, Loss: 4.370050430297852\n",
      "Epoch 1, Loss: 3.5256972312927246\n",
      "Epoch 1, Loss: 4.500533103942871\n",
      "Epoch 1, Loss: 4.2622599601745605\n",
      "Epoch 1, Loss: 4.798432350158691\n",
      "Epoch 1, Loss: 3.8473093509674072\n",
      "Epoch 1, Loss: 4.771410942077637\n",
      "Epoch 1, Loss: 3.677469253540039\n",
      "Epoch 1, Loss: 4.109455585479736\n",
      "Epoch 1, Loss: 3.9634411334991455\n",
      "Epoch 1, Loss: 4.365016937255859\n",
      "Epoch 1, Loss: 3.616006851196289\n",
      "Epoch 1, Loss: 4.3409223556518555\n",
      "Epoch 1, Loss: 2.5730228424072266\n",
      "Epoch 1, Loss: 3.504615306854248\n",
      "Epoch 1, Loss: 4.497931480407715\n",
      "Epoch 1, Loss: 3.977457046508789\n",
      "Epoch 1, Loss: 4.248747825622559\n",
      "Epoch 1, Loss: 4.238710403442383\n",
      "Epoch 1, Loss: 5.011981964111328\n",
      "Epoch 1, Loss: 3.797065496444702\n",
      "Epoch 1, Loss: 3.807129383087158\n",
      "Epoch 1, Loss: 3.460019111633301\n",
      "Epoch 1, Loss: 4.121642589569092\n",
      "Epoch 1, Loss: 2.9343438148498535\n",
      "Epoch 1, Loss: 4.12175178527832\n",
      "Epoch 1, Loss: 4.129423141479492\n",
      "Epoch 1, Loss: 3.8727447986602783\n",
      "Epoch 1, Loss: 3.681804656982422\n",
      "Epoch 1, Loss: 3.9958596229553223\n",
      "Epoch 1, Loss: 4.873178482055664\n",
      "Epoch 1, Loss: 4.618669509887695\n",
      "Epoch 1, Loss: 4.462719917297363\n",
      "Epoch 1, Loss: 4.2225141525268555\n",
      "Epoch 1, Loss: 4.2770562171936035\n",
      "Epoch 1, Loss: 3.3760416507720947\n",
      "Epoch 1, Loss: 3.8082258701324463\n",
      "Epoch 1, Loss: 4.130645751953125\n",
      "Epoch 1, Loss: 5.136395454406738\n",
      "Epoch 1, Loss: 5.353994369506836\n",
      "Epoch 1, Loss: 4.18461799621582\n",
      "Epoch 1, Loss: 4.050656318664551\n",
      "Epoch 1, Loss: 3.7170093059539795\n",
      "Epoch 1, Loss: 4.234951019287109\n",
      "Epoch 1, Loss: 4.69492244720459\n",
      "Epoch 1, Loss: 4.170936584472656\n",
      "Epoch 1, Loss: 4.674714088439941\n",
      "Epoch 1, Loss: 4.783195495605469\n",
      "Epoch 1, Loss: 3.33333683013916\n",
      "Epoch 1, Loss: 3.8605475425720215\n",
      "Epoch 1, Loss: 4.000351905822754\n",
      "Epoch 1, Loss: 3.9996447563171387\n",
      "Epoch 1, Loss: 3.082704782485962\n",
      "Epoch 1, Loss: 3.86549711227417\n",
      "Epoch 1, Loss: 5.1528472900390625\n",
      "Epoch 1, Loss: 4.368827819824219\n",
      "Epoch 1, Loss: 3.864293098449707\n",
      "Epoch 1, Loss: 3.7091174125671387\n",
      "Epoch 1, Loss: 4.382393836975098\n",
      "Epoch 1, Loss: 3.024966239929199\n",
      "Epoch 1, Loss: 4.003012657165527\n",
      "Epoch 1, Loss: 4.0812273025512695\n",
      "Epoch 1, Loss: 4.510797500610352\n",
      "Epoch 1, Loss: 3.855180263519287\n",
      "Epoch 1, Loss: 4.500319480895996\n",
      "Epoch 1, Loss: 4.500123977661133\n",
      "Epoch 1, Loss: 4.475054740905762\n",
      "Epoch 1, Loss: 4.180120944976807\n",
      "Epoch 1, Loss: 3.2901740074157715\n",
      "Epoch 1, Loss: 3.7543039321899414\n",
      "Epoch 1, Loss: 3.555809497833252\n",
      "Epoch 1, Loss: 4.115512847900391\n",
      "Epoch 1, Loss: 2.8461813926696777\n",
      "Epoch 1, Loss: 3.589430809020996\n",
      "Epoch 1, Loss: 3.670304298400879\n",
      "Epoch 1, Loss: 3.79819393157959\n",
      "Epoch 1, Loss: 4.23079776763916\n",
      "Epoch 1, Loss: 3.6002988815307617\n",
      "Epoch 1, Loss: 2.6657729148864746\n",
      "Epoch 1, Loss: 3.371960401535034\n",
      "Epoch 1, Loss: 3.3861942291259766\n",
      "Epoch 1, Loss: 3.9765877723693848\n",
      "Epoch 1, Loss: 4.061901569366455\n",
      "Epoch 1, Loss: 4.4912109375\n",
      "Epoch 1, Loss: 3.2423009872436523\n",
      "Epoch 1, Loss: 4.115105628967285\n",
      "Epoch 1, Loss: 4.958056449890137\n",
      "Epoch 1, Loss: 3.460026979446411\n",
      "Epoch 1, Loss: 4.1178669929504395\n",
      "Epoch 1, Loss: 3.86738920211792\n",
      "Epoch 1, Loss: 4.023104190826416\n",
      "Epoch 1, Loss: 3.5664167404174805\n",
      "Epoch 1, Loss: 3.494135856628418\n",
      "Epoch 1, Loss: 4.019717216491699\n",
      "Epoch 1, Loss: 5.082622051239014\n",
      "Epoch 1, Loss: 5.026246547698975\n",
      "Epoch 1, Loss: 3.8144307136535645\n",
      "Epoch 1, Loss: 4.5314788818359375\n",
      "Epoch 1, Loss: 4.121654510498047\n",
      "Epoch 1, Loss: 4.419998645782471\n",
      "Epoch 1, Loss: 3.457259178161621\n",
      "Epoch 1, Loss: 3.3784172534942627\n",
      "Epoch 1, Loss: 4.562166213989258\n",
      "Epoch 1, Loss: 4.193514823913574\n",
      "Epoch 1, Loss: 3.07437801361084\n",
      "Epoch 1, Loss: 4.623970031738281\n",
      "Epoch 1, Loss: 3.9073500633239746\n",
      "Epoch 1, Loss: 4.31494140625\n",
      "Epoch 1, Loss: 4.857205390930176\n",
      "Epoch 1, Loss: 3.7555084228515625\n",
      "Epoch 1, Loss: 3.7737927436828613\n",
      "Epoch 1, Loss: 3.1028356552124023\n",
      "Epoch 1, Loss: 4.268874168395996\n",
      "Epoch 1, Loss: 4.212956428527832\n",
      "Epoch 1, Loss: 3.6231274604797363\n",
      "Epoch 1, Loss: 4.02168083190918\n",
      "Epoch 1, Loss: 2.8050224781036377\n",
      "Epoch 1, Loss: 3.8526229858398438\n",
      "Epoch 1, Loss: 4.081934452056885\n",
      "Epoch 1, Loss: 3.727656841278076\n",
      "Epoch 1, Loss: 4.625186920166016\n",
      "Epoch 1, Loss: 3.154369592666626\n",
      "Epoch 1, Loss: 3.2483277320861816\n",
      "Epoch 1, Loss: 5.324013710021973\n",
      "Epoch 1, Loss: 3.408637046813965\n",
      "Epoch 1, Loss: 4.536784648895264\n",
      "Epoch 1, Loss: 3.8295626640319824\n",
      "Epoch 1, Loss: 3.024702310562134\n",
      "Epoch 1, Loss: 3.147383213043213\n",
      "Epoch 1, Loss: 2.8327126502990723\n",
      "Epoch 1, Loss: 3.3596673011779785\n",
      "Epoch 1, Loss: 4.1534576416015625\n",
      "Epoch 1, Loss: 3.659250259399414\n",
      "Epoch 1, Loss: 3.4120237827301025\n",
      "Epoch 1, Loss: 3.8178720474243164\n",
      "Epoch 1, Loss: 3.616194725036621\n",
      "Epoch 1, Loss: 4.410696029663086\n",
      "Epoch 1, Loss: 2.9060347080230713\n",
      "Epoch 1, Loss: 3.6358091831207275\n",
      "Epoch 1, Loss: 4.711763858795166\n",
      "Epoch 1, Loss: 3.3130345344543457\n",
      "Epoch 1, Loss: 2.8239247798919678\n",
      "Epoch 1, Loss: 3.456514835357666\n",
      "Epoch 1, Loss: 3.4161510467529297\n",
      "Epoch 1, Loss: 2.7607991695404053\n",
      "Epoch 1, Loss: 3.1035072803497314\n",
      "Epoch 1, Loss: 1.9912750720977783\n",
      "Epoch 1, Loss: 3.1634247303009033\n",
      "Epoch 1, Loss: 3.180300712585449\n",
      "Epoch 1, Loss: 4.763285160064697\n",
      "Epoch 1, Loss: 3.528010368347168\n",
      "Epoch 1, Loss: 3.3851165771484375\n",
      "Epoch 1, Loss: 5.438804626464844\n",
      "Epoch 1, Loss: 3.23252534866333\n",
      "Epoch 1, Loss: 2.9167418479919434\n",
      "Epoch 1, Loss: 4.148416519165039\n",
      "Epoch 1, Loss: 4.143745422363281\n",
      "Epoch 1, Loss: 4.838090896606445\n",
      "Epoch 1, Loss: 4.016740798950195\n",
      "Epoch 1, Loss: 2.635108470916748\n",
      "Epoch 1, Loss: 4.452577590942383\n",
      "Epoch 1, Loss: 3.1910622119903564\n",
      "Epoch 1, Loss: 4.087102890014648\n",
      "Epoch 1, Loss: 4.155272483825684\n",
      "Epoch 1, Loss: 3.35617733001709\n",
      "Epoch 1, Loss: 2.6196866035461426\n",
      "Epoch 1, Loss: 3.1602697372436523\n",
      "Epoch 1, Loss: 5.230550289154053\n",
      "Epoch 1, Loss: 2.9108779430389404\n",
      "Epoch 1, Loss: 4.268942832946777\n",
      "Epoch 1, Loss: 4.447907447814941\n",
      "Epoch 1, Loss: 3.822291612625122\n",
      "Epoch 1, Loss: 4.152740955352783\n",
      "Epoch 1, Loss: 2.832106351852417\n",
      "Epoch 1, Loss: 3.5895299911499023\n",
      "Epoch 1, Loss: 2.93900728225708\n",
      "Epoch 1, Loss: 3.66611647605896\n",
      "Epoch 1, Loss: 3.373868703842163\n",
      "Epoch 1, Loss: 3.1171491146087646\n",
      "Epoch 1, Loss: 2.540088176727295\n",
      "Epoch 1, Loss: 3.794020175933838\n",
      "Epoch 1, Loss: 2.95292592048645\n",
      "Epoch 1, Loss: 3.391094207763672\n",
      "Epoch 1, Loss: 3.3894765377044678\n",
      "Epoch 1, Loss: 3.507195472717285\n",
      "Epoch 1, Loss: 3.967428684234619\n",
      "Epoch 1, Loss: 4.07009220123291\n",
      "Epoch 1, Loss: 3.3896234035491943\n",
      "Epoch 1, Loss: 3.2491283416748047\n",
      "Epoch 1, Loss: 3.686119794845581\n",
      "Epoch 1, Loss: 4.490336894989014\n",
      "Epoch 1, Loss: 4.068279266357422\n",
      "Epoch 1, Loss: 3.068748712539673\n",
      "Epoch 1, Loss: 3.5583930015563965\n",
      "Epoch 1, Loss: 1.6006463766098022\n",
      "Epoch 1, Loss: 4.038639545440674\n",
      "Epoch 1, Loss: 2.2971861362457275\n",
      "Epoch 1, Loss: 4.406493186950684\n",
      "Epoch 1, Loss: 3.822646141052246\n",
      "Epoch 1, Loss: 4.030814170837402\n",
      "Epoch 1, Loss: 3.6858978271484375\n",
      "Epoch 1, Loss: 3.680663824081421\n",
      "Epoch 1, Loss: 4.037202835083008\n",
      "Epoch 1, Loss: 3.431992530822754\n",
      "Epoch 1, Loss: 3.7999515533447266\n",
      "Epoch 1, Loss: 3.634157180786133\n",
      "Epoch 1, Loss: 2.6646313667297363\n",
      "Epoch 1, Loss: 3.345475673675537\n",
      "Epoch 1, Loss: 4.184929370880127\n",
      "Epoch 1, Loss: 3.310549736022949\n",
      "Epoch 1, Loss: 4.6583709716796875\n",
      "Epoch 1, Loss: 3.671581268310547\n",
      "Epoch 1, Loss: 4.045467376708984\n",
      "Epoch 1, Loss: 3.5135998725891113\n",
      "Epoch 1, Loss: 3.148686408996582\n",
      "Epoch 1, Loss: 3.7971696853637695\n",
      "Epoch 1, Loss: 3.5031213760375977\n",
      "Epoch 1, Loss: 3.607333183288574\n",
      "Epoch 1, Loss: 4.369309425354004\n",
      "Epoch 1, Loss: 3.199878692626953\n",
      "Epoch 1, Loss: 3.786609411239624\n",
      "Epoch 1, Loss: 3.5626273155212402\n",
      "Epoch 1, Loss: 4.063845157623291\n",
      "Epoch 1, Loss: 3.3552980422973633\n",
      "Epoch 1, Loss: 1.8634817600250244\n",
      "Epoch 1, Loss: 3.3818397521972656\n",
      "Epoch 1, Loss: 4.426556587219238\n",
      "Epoch 1, Loss: 3.147425651550293\n",
      "Epoch 1, Loss: 3.7725319862365723\n",
      "Epoch 1, Loss: 3.533628463745117\n",
      "Epoch 1, Loss: 3.98751163482666\n",
      "Epoch 1, Loss: 4.023083686828613\n",
      "Epoch 1, Loss: 3.4247889518737793\n",
      "Epoch 1, Loss: 4.40109395980835\n",
      "Epoch 1, Loss: 2.3012938499450684\n",
      "Epoch 1, Loss: 3.534942865371704\n",
      "Epoch 1, Loss: 3.1818041801452637\n",
      "Epoch 1, Loss: 3.019929885864258\n",
      "Epoch 1, Loss: 3.567627429962158\n",
      "Epoch 1, Loss: 3.7057766914367676\n",
      "Epoch 1, Loss: 4.083030700683594\n",
      "Epoch 1, Loss: 3.9634339809417725\n",
      "Epoch 1, Loss: 3.371551990509033\n",
      "Epoch 1, Loss: 4.065216541290283\n",
      "Epoch 1, Loss: 4.64177131652832\n",
      "Epoch 1, Loss: 3.6743783950805664\n",
      "Epoch 1, Loss: 4.181527137756348\n",
      "Epoch 1, Loss: 3.6295413970947266\n",
      "Epoch 1, Loss: 2.970229148864746\n",
      "Epoch 1, Loss: 3.9334640502929688\n",
      "Epoch 1, Loss: 4.627447128295898\n",
      "Epoch 1, Loss: 4.2078633308410645\n",
      "Epoch 1, Loss: 4.170284271240234\n",
      "Epoch 1, Loss: 3.688386917114258\n",
      "Epoch 1, Loss: 3.07778263092041\n",
      "Epoch 1, Loss: 4.01103401184082\n",
      "Epoch 1, Loss: 3.8191232681274414\n",
      "Epoch 1, Loss: 3.3727455139160156\n",
      "Epoch 1, Loss: 4.066253185272217\n",
      "Epoch 1, Loss: 3.6638307571411133\n",
      "Epoch 1, Loss: 3.3839237689971924\n",
      "Epoch 1, Loss: 4.121766090393066\n",
      "Epoch 1, Loss: 3.7227182388305664\n",
      "Epoch 1, Loss: 2.8722314834594727\n",
      "Epoch 1, Loss: 2.2650279998779297\n",
      "Epoch 1, Loss: 3.294109344482422\n",
      "Epoch 1, Loss: 3.189037561416626\n",
      "Epoch 1, Loss: 3.9887497425079346\n",
      "Epoch 1, Loss: 4.564536094665527\n",
      "Epoch 1, Loss: 2.490530490875244\n",
      "Epoch 1, Loss: 4.015276908874512\n",
      "Epoch 1, Loss: 3.8747286796569824\n",
      "Epoch 1, Loss: 3.786874771118164\n",
      "Epoch 1, Loss: 3.8107967376708984\n",
      "Epoch 1, Loss: 3.8414716720581055\n",
      "Epoch 1, Loss: 2.94271183013916\n",
      "Epoch 1, Loss: 2.495896816253662\n",
      "Epoch 1, Loss: 3.3256008625030518\n",
      "Epoch 1, Loss: 2.9634451866149902\n",
      "Epoch 1, Loss: 3.6815388202667236\n",
      "Epoch 1, Loss: 3.472008228302002\n",
      "Epoch 1, Loss: 3.0605685710906982\n",
      "Epoch 1, Loss: 4.263505458831787\n",
      "Epoch 1, Loss: 2.7279844284057617\n",
      "Epoch 1, Loss: 3.32073712348938\n",
      "Epoch 1, Loss: 3.73514461517334\n",
      "Epoch 1, Loss: 4.0591349601745605\n",
      "Epoch 1, Loss: 3.3726792335510254\n",
      "Epoch 1, Loss: 2.745776653289795\n",
      "Epoch 1, Loss: 2.7019433975219727\n",
      "Epoch 1, Loss: 3.3632607460021973\n",
      "Epoch 1, Loss: 3.5381407737731934\n",
      "Epoch 1, Loss: 2.631849765777588\n",
      "Epoch 1, Loss: 3.824406862258911\n",
      "Epoch 1, Loss: 3.687074661254883\n",
      "Epoch 1, Loss: 3.6335349082946777\n",
      "Epoch 1, Loss: 3.9730844497680664\n",
      "Epoch 1, Loss: 2.9225850105285645\n",
      "Epoch 1, Loss: 3.155146360397339\n",
      "Epoch 1, Loss: 3.5801429748535156\n",
      "Epoch 1, Loss: 2.4321365356445312\n",
      "Epoch 1, Loss: 3.042278289794922\n",
      "Epoch 1, Loss: 2.8607988357543945\n",
      "Epoch 1, Loss: 3.8901185989379883\n",
      "Epoch 1, Loss: 2.921572208404541\n",
      "Epoch 1, Loss: 3.5977954864501953\n",
      "Epoch 1, Loss: 2.0211691856384277\n",
      "Epoch 1, Loss: 2.1496665477752686\n",
      "Epoch 1, Loss: 1.4398915767669678\n",
      "Epoch 1, Loss: 2.8475544452667236\n",
      "Epoch 1, Loss: 4.121389865875244\n",
      "Epoch 1, Loss: 3.4596686363220215\n",
      "Epoch 1, Loss: 3.721194267272949\n",
      "Epoch 1, Loss: 3.241887092590332\n",
      "Epoch 1, Loss: 3.8209714889526367\n",
      "Epoch 1, Loss: 3.142822265625\n",
      "Epoch 1, Loss: 2.560333251953125\n",
      "Epoch 1, Loss: 2.2564005851745605\n",
      "Epoch 1, Loss: 3.1082205772399902\n",
      "Epoch 1, Loss: 3.4917006492614746\n",
      "Epoch 1, Loss: 3.5900354385375977\n",
      "Epoch 1, Loss: 3.06197190284729\n",
      "Epoch 1, Loss: 4.386195182800293\n",
      "Epoch 1, Loss: 3.6613621711730957\n",
      "Epoch 1, Loss: 3.308908462524414\n",
      "Epoch 1, Loss: 2.64676570892334\n",
      "Epoch 1, Loss: 4.717437744140625\n",
      "Epoch 1, Loss: 3.481020927429199\n",
      "Epoch 1, Loss: 3.2879624366760254\n",
      "Epoch 1, Loss: 3.148693561553955\n",
      "Epoch 1, Loss: 3.864220380783081\n",
      "Epoch 1, Loss: 4.110543251037598\n",
      "Epoch 1, Loss: 3.221477508544922\n",
      "Epoch 1, Loss: 3.9889917373657227\n",
      "Epoch 1, Loss: 3.444890022277832\n",
      "Epoch 1, Loss: 3.6039927005767822\n",
      "Epoch 1, Loss: 2.8810455799102783\n",
      "Epoch 1, Loss: 3.292689561843872\n",
      "Epoch 1, Loss: 3.935340404510498\n",
      "Epoch 1, Loss: 3.574282646179199\n",
      "Epoch 1, Loss: 2.796658992767334\n",
      "Epoch 1, Loss: 3.2907493114471436\n",
      "Epoch 1, Loss: 3.5581107139587402\n",
      "Epoch 1, Loss: 2.6289939880371094\n",
      "Epoch 1, Loss: 3.52476167678833\n",
      "Epoch 1, Loss: 4.465293884277344\n",
      "Epoch 1, Loss: 1.9359986782073975\n",
      "Epoch 1, Loss: 3.1810123920440674\n",
      "Epoch 1, Loss: 3.9256558418273926\n",
      "Epoch 1, Loss: 2.871721029281616\n",
      "Epoch 1, Loss: 3.762495994567871\n",
      "Epoch 1, Loss: 3.1317310333251953\n",
      "Epoch 1, Loss: 3.0846071243286133\n",
      "Epoch 1, Loss: 3.694638252258301\n",
      "Epoch 1, Loss: 2.69329833984375\n",
      "Epoch 1, Loss: 3.592165470123291\n",
      "Epoch 1, Loss: 3.6014761924743652\n",
      "Epoch 1, Loss: 3.3919434547424316\n",
      "Epoch 1, Loss: 2.4473886489868164\n",
      "Epoch 1, Loss: 3.644552230834961\n",
      "Epoch 1, Loss: 2.5220375061035156\n",
      "Epoch 1, Loss: 3.0950498580932617\n",
      "Epoch 1, Loss: 3.347947597503662\n",
      "Epoch 1, Loss: 3.4268670082092285\n",
      "Epoch 1, Loss: 3.2778830528259277\n",
      "Epoch 1, Loss: 4.080282688140869\n",
      "Epoch 1, Loss: 2.6205601692199707\n",
      "Epoch 1, Loss: 4.397512435913086\n",
      "Epoch 1, Loss: 3.6450414657592773\n",
      "Epoch 1, Loss: 2.530623435974121\n",
      "Epoch 1, Loss: 3.230681896209717\n",
      "Epoch 1, Loss: 3.1843199729919434\n",
      "Epoch 1, Loss: 3.4245944023132324\n",
      "Epoch 1, Loss: 3.1250882148742676\n",
      "Epoch 1, Loss: 3.2387232780456543\n",
      "Epoch 1, Loss: 4.1732258796691895\n",
      "Epoch 1, Loss: 3.2750234603881836\n",
      "Epoch 1, Loss: 2.845107078552246\n",
      "Epoch 1, Loss: 4.111160755157471\n",
      "Epoch 1, Loss: 2.6062657833099365\n",
      "Epoch 1, Loss: 2.810194730758667\n",
      "Epoch 1, Loss: 2.46763277053833\n",
      "Epoch 1, Loss: 4.0238213539123535\n",
      "Epoch 1, Loss: 2.55730938911438\n",
      "Epoch 1, Loss: 2.767648935317993\n",
      "Epoch 1, Loss: 4.130773544311523\n",
      "Epoch 1, Loss: 2.3512961864471436\n",
      "Epoch 1, Loss: 2.257661819458008\n",
      "Epoch 1, Loss: 2.4904234409332275\n",
      "Epoch 1, Loss: 3.758563995361328\n",
      "Epoch 1, Loss: 2.434873580932617\n",
      "Epoch 1, Loss: 3.9385976791381836\n",
      "Epoch 1, Loss: 3.665923833847046\n",
      "Epoch 1, Loss: 3.3806159496307373\n",
      "Epoch 1, Loss: 4.800335884094238\n",
      "Epoch 1, Loss: 1.9895797967910767\n",
      "Epoch 1, Loss: 2.6865084171295166\n",
      "Epoch 1, Loss: 4.037939071655273\n",
      "Epoch 1, Loss: 3.4378645420074463\n",
      "Epoch 1, Loss: 2.6590020656585693\n",
      "Epoch 1, Loss: 3.427304744720459\n",
      "Epoch 1, Loss: 3.2734103202819824\n",
      "Epoch 1, Loss: 3.556034564971924\n",
      "Epoch 1, Loss: 3.9216628074645996\n",
      "Epoch 1, Loss: 4.629550933837891\n",
      "Epoch 1, Loss: 2.808785915374756\n",
      "Epoch 1, Loss: 3.1530184745788574\n",
      "Epoch 1, Loss: 3.5464248657226562\n",
      "Epoch 1, Loss: 3.414992332458496\n",
      "Epoch 1, Loss: 3.3473644256591797\n",
      "Epoch 1, Loss: 2.800851821899414\n",
      "Epoch 1, Loss: 3.2357237339019775\n",
      "Epoch 1, Loss: 3.282393217086792\n",
      "Epoch 1, Loss: 2.3860297203063965\n",
      "Epoch 1, Loss: 2.3179821968078613\n",
      "Epoch 1, Loss: 2.922177791595459\n",
      "Epoch 1, Loss: 2.894756317138672\n",
      "Epoch 1, Loss: 2.9968934059143066\n",
      "Epoch 1, Loss: 3.544124126434326\n",
      "Epoch 1, Loss: 2.5960960388183594\n",
      "Epoch 1, Loss: 2.9488139152526855\n",
      "Epoch 1, Loss: 3.0355734825134277\n",
      "Epoch 1, Loss: 4.019948959350586\n",
      "Epoch 1, Loss: 1.5084953308105469\n",
      "Epoch 1, Loss: 2.8334360122680664\n",
      "Epoch 1, Loss: 2.9150195121765137\n",
      "Epoch 1, Loss: 3.4927778244018555\n",
      "Epoch 1, Loss: 2.6514201164245605\n",
      "Epoch 1, Loss: 2.7127909660339355\n",
      "Epoch 1, Loss: 2.707683563232422\n",
      "Epoch 1, Loss: 2.7689104080200195\n",
      "Epoch 1, Loss: 2.3559398651123047\n",
      "Epoch 1, Loss: 2.927358865737915\n",
      "Epoch 1, Loss: 3.2218170166015625\n",
      "Epoch 1, Loss: 3.1334614753723145\n",
      "Epoch 1, Loss: 3.4332847595214844\n",
      "Epoch 1, Loss: 3.9234957695007324\n",
      "Epoch 1, Loss: 3.041027307510376\n",
      "Epoch 1, Loss: 3.3419017791748047\n",
      "Epoch 1, Loss: 2.9691226482391357\n",
      "Epoch 1, Loss: 2.729140520095825\n",
      "Epoch 1, Loss: 3.976449966430664\n",
      "Epoch 1, Loss: 2.677946090698242\n",
      "Epoch 1, Loss: 3.5671305656433105\n",
      "Epoch 1, Loss: 2.799427032470703\n",
      "Epoch 1, Loss: 2.5330252647399902\n",
      "Epoch 1, Loss: 2.216578960418701\n",
      "Epoch 1, Loss: 2.5936543941497803\n",
      "Epoch 1, Loss: 2.8123960494995117\n",
      "Epoch 1, Loss: 2.489989757537842\n",
      "Epoch 1, Loss: 4.198123931884766\n",
      "Epoch 1, Loss: 2.7001595497131348\n",
      "Epoch 1, Loss: 1.8418298959732056\n",
      "Epoch 1, Loss: 2.888333797454834\n",
      "Epoch 1, Loss: 2.6933963298797607\n",
      "Epoch 1, Loss: 1.8182094097137451\n",
      "Epoch 1, Loss: 2.0427560806274414\n",
      "Epoch 1, Loss: 4.199423789978027\n",
      "Epoch 1, Loss: 3.1808714866638184\n",
      "Epoch 1, Loss: 3.7497286796569824\n",
      "Epoch 1, Loss: 2.005977153778076\n",
      "Epoch 1, Loss: 3.5146775245666504\n",
      "Epoch 1, Loss: 3.222352981567383\n",
      "Epoch 1, Loss: 2.2491631507873535\n",
      "Epoch 1, Loss: 3.6418442726135254\n",
      "Epoch 1, Loss: 2.240777015686035\n",
      "Epoch 1, Loss: 3.314337730407715\n",
      "Epoch 1, Loss: 2.2731456756591797\n",
      "Epoch 1, Loss: 2.5482473373413086\n",
      "Epoch 1, Loss: 2.7596607208251953\n",
      "Epoch 1, Loss: 3.165435314178467\n",
      "Epoch 1, Loss: 2.418309211730957\n",
      "Epoch 1, Loss: 2.192689895629883\n",
      "Epoch 1, Loss: 1.8886303901672363\n",
      "Epoch 1, Loss: 2.4224014282226562\n",
      "Epoch 1, Loss: 1.800778865814209\n",
      "Epoch 1, Loss: 3.5522818565368652\n",
      "Epoch 1, Loss: 3.442086696624756\n",
      "Epoch 1, Loss: 3.42158842086792\n",
      "Epoch 1, Loss: 3.2389798164367676\n",
      "Epoch 1, Loss: 4.647902965545654\n",
      "Epoch 1, Loss: 2.652273178100586\n",
      "Epoch 1, Loss: 3.7280044555664062\n",
      "Epoch 1, Loss: 3.278223991394043\n",
      "Epoch 1, Loss: 3.1899521350860596\n",
      "Epoch 1, Loss: 2.2870590686798096\n",
      "Epoch 1, Loss: 2.1267504692077637\n",
      "Epoch 1, Loss: 2.4092607498168945\n",
      "Epoch 1, Loss: 2.911125659942627\n",
      "Epoch 1, Loss: 3.592158317565918\n",
      "Epoch 1, Loss: 3.44679856300354\n",
      "Epoch 1, Loss: 3.1599373817443848\n",
      "Epoch 1, Loss: 3.2607665061950684\n",
      "Epoch 1, Loss: 2.808516025543213\n",
      "Epoch 1, Loss: 2.960292339324951\n",
      "Epoch 1, Loss: 3.4426302909851074\n",
      "Epoch 1, Loss: 2.722216844558716\n",
      "Epoch 1, Loss: 2.5204813480377197\n",
      "Epoch 1, Loss: 3.258979320526123\n",
      "Epoch 1, Loss: 2.9776065349578857\n",
      "Epoch 1, Loss: 4.1013593673706055\n",
      "Epoch 1, Loss: 3.7791600227355957\n",
      "Epoch 1, Loss: 3.3584744930267334\n",
      "Epoch 1, Loss: 3.3773794174194336\n",
      "Epoch 1, Loss: 2.655093193054199\n",
      "Epoch 1, Loss: 2.8145320415496826\n",
      "Epoch 1, Loss: 3.5247139930725098\n",
      "Epoch 1, Loss: 2.7125682830810547\n",
      "Epoch 1, Loss: 2.573457956314087\n",
      "Epoch 1, Loss: 3.6783580780029297\n",
      "Epoch 1, Loss: 2.1201982498168945\n",
      "Epoch 1, Loss: 3.5755202770233154\n",
      "Epoch 1, Loss: 3.0631790161132812\n",
      "Epoch 1, Loss: 2.895338296890259\n",
      "Epoch 1, Loss: 3.0841965675354004\n",
      "Epoch 1, Loss: 3.0897104740142822\n",
      "Epoch 1, Loss: 3.449951648712158\n",
      "Epoch 1, Loss: 2.535388469696045\n",
      "Epoch 1, Loss: 3.4570517539978027\n",
      "Epoch 1, Loss: 3.3125810623168945\n",
      "Epoch 1, Loss: 2.816464900970459\n",
      "Epoch 1, Loss: 3.5495777130126953\n",
      "Epoch 1, Loss: 1.9309743642807007\n",
      "Epoch 1, Loss: 3.4461185932159424\n",
      "Epoch 1, Loss: 2.577934741973877\n",
      "Epoch 1, Loss: 3.339357376098633\n",
      "Epoch 1, Loss: 2.8663816452026367\n",
      "Epoch 1, Loss: 2.49312162399292\n",
      "Epoch 1, Loss: 3.0757761001586914\n",
      "Epoch 1, Loss: 2.2808918952941895\n",
      "Epoch 1, Loss: 3.0087690353393555\n",
      "Epoch 1, Loss: 2.3869636058807373\n",
      "Epoch 1, Loss: 2.601719856262207\n",
      "Epoch 1, Loss: 2.6860828399658203\n",
      "Epoch 1, Loss: 3.501023769378662\n",
      "Epoch 1, Loss: 3.2870702743530273\n",
      "Epoch 1, Loss: 2.8765361309051514\n",
      "Epoch 1, Loss: 2.4415225982666016\n",
      "Epoch 1, Loss: 2.9112613201141357\n",
      "Epoch 1, Loss: 3.4189462661743164\n",
      "Epoch 1, Loss: 2.196650981903076\n",
      "Epoch 1, Loss: 3.531505584716797\n",
      "Epoch 1, Loss: 2.72963285446167\n",
      "Epoch 1, Loss: 3.1694533824920654\n",
      "Epoch 1, Loss: 2.7329182624816895\n",
      "Epoch 1, Loss: 2.6212496757507324\n",
      "Epoch 1, Loss: 2.97860050201416\n",
      "Epoch 1, Loss: 1.8024097681045532\n",
      "Epoch 1, Loss: 4.860953330993652\n",
      "Epoch 1, Loss: 2.581432819366455\n",
      "Epoch 1, Loss: 2.633918285369873\n",
      "Epoch 1, Loss: 3.2827868461608887\n",
      "Epoch 1, Loss: 2.4716286659240723\n",
      "Epoch 1, Loss: 2.8467116355895996\n",
      "Epoch 1, Loss: 2.3184046745300293\n",
      "Epoch 1, Loss: 3.224491834640503\n",
      "Epoch 1, Loss: 2.5832295417785645\n",
      "Epoch 1, Loss: 3.612396240234375\n",
      "Epoch 1, Loss: 2.7043051719665527\n",
      "Epoch 1, Loss: 2.5201237201690674\n",
      "Epoch 1, Loss: 1.8060133457183838\n",
      "Epoch 1, Loss: 3.4246270656585693\n",
      "Epoch 1, Loss: 2.595557689666748\n",
      "Epoch 1, Loss: 2.656527519226074\n",
      "Epoch 1, Loss: 3.3080363273620605\n",
      "Epoch 1, Loss: 2.6145589351654053\n",
      "Epoch 1, Loss: 3.373128652572632\n",
      "Epoch 1, Loss: 3.439791202545166\n",
      "Epoch 1, Loss: 2.325446128845215\n",
      "Epoch 1, Loss: 2.902360200881958\n",
      "Epoch 1, Loss: 2.7629990577697754\n",
      "Epoch 1, Loss: 2.808711051940918\n",
      "Epoch 1, Loss: 2.9268014430999756\n",
      "Epoch 1, Loss: 1.9322504997253418\n",
      "Epoch 1, Loss: 3.557036876678467\n",
      "Epoch 1, Loss: 3.1040453910827637\n",
      "Epoch 1, Loss: 3.490703821182251\n",
      "Epoch 1, Loss: 2.061309814453125\n",
      "Epoch 1, Loss: 3.2763571739196777\n",
      "Epoch 1, Loss: 2.8814682960510254\n",
      "Epoch 1, Loss: 2.348050117492676\n",
      "Epoch 1, Loss: 3.1545608043670654\n",
      "Epoch 1, Loss: 2.7184736728668213\n",
      "Epoch 1, Loss: 3.1236448287963867\n",
      "Epoch 1, Loss: 2.4762966632843018\n",
      "Epoch 1, Loss: 3.8942301273345947\n",
      "Epoch 1, Loss: 2.4055235385894775\n",
      "Epoch 1, Loss: 3.279033899307251\n",
      "Epoch 1, Loss: 3.1741042137145996\n",
      "Epoch 1, Loss: 2.264148235321045\n",
      "Epoch 1, Loss: 2.818044662475586\n",
      "Epoch 1, Loss: 3.2471771240234375\n",
      "Epoch 1, Loss: 2.882950782775879\n",
      "Epoch 1, Loss: 3.0509066581726074\n",
      "Epoch 1, Loss: 2.67307710647583\n",
      "Epoch 1, Loss: 2.8309199810028076\n",
      "Epoch 1, Loss: 3.4408774375915527\n",
      "Epoch 1, Loss: 1.9041560888290405\n",
      "Epoch 1, Loss: 1.977001667022705\n",
      "Epoch 1, Loss: 2.3494277000427246\n",
      "Epoch 1, Loss: 2.629514694213867\n",
      "Epoch 1, Loss: 2.573486089706421\n",
      "Epoch 1, Loss: 2.1963508129119873\n",
      "Epoch 1, Loss: 3.1106417179107666\n",
      "Epoch 1, Loss: 3.0384984016418457\n",
      "Epoch 1, Loss: 3.4924261569976807\n",
      "Epoch 1, Loss: 2.093726396560669\n",
      "Epoch 1, Loss: 2.488715171813965\n",
      "Epoch 1, Loss: 1.9583563804626465\n",
      "Epoch 1, Loss: 3.662647008895874\n",
      "Epoch 1, Loss: 3.513382911682129\n",
      "Epoch 1, Loss: 3.346588611602783\n",
      "Epoch 1, Loss: 3.473008155822754\n",
      "Epoch 1, Loss: 2.8104729652404785\n",
      "Epoch 1, Loss: 2.9767613410949707\n",
      "Epoch 1, Loss: 2.6402063369750977\n",
      "Epoch 1, Loss: 3.2054080963134766\n",
      "Epoch 1, Loss: 2.60983943939209\n",
      "Epoch 1, Loss: 3.2748587131500244\n",
      "Epoch 1, Loss: 3.0793018341064453\n",
      "Epoch 1, Loss: 3.514310598373413\n",
      "Epoch 1, Loss: 2.7155041694641113\n",
      "Epoch 1, Loss: 2.467012882232666\n",
      "Epoch 1, Loss: 2.353119134902954\n",
      "Epoch 1, Loss: 1.8691532611846924\n",
      "Epoch 1, Loss: 2.5050604343414307\n",
      "Epoch 1, Loss: 2.6436221599578857\n",
      "Epoch 1, Loss: 2.5750598907470703\n",
      "Epoch 1, Loss: 2.29486083984375\n",
      "Epoch 1, Loss: 3.691209316253662\n",
      "Epoch 1, Loss: 2.4417219161987305\n",
      "Epoch 1, Loss: 2.0293235778808594\n",
      "Epoch 1, Loss: 2.736426830291748\n",
      "Epoch 1, Loss: 2.2893288135528564\n",
      "Epoch 1, Loss: 3.0881729125976562\n",
      "Epoch 1, Loss: 3.323744297027588\n",
      "Epoch 1, Loss: 2.640291690826416\n",
      "Epoch 1, Loss: 3.4461560249328613\n",
      "Epoch 1, Loss: 1.2842576503753662\n",
      "Epoch 1, Loss: 2.1151247024536133\n",
      "Epoch 1, Loss: 2.303386688232422\n",
      "Epoch 1, Loss: 2.7473278045654297\n",
      "Epoch 1, Loss: 2.1711771488189697\n",
      "Epoch 1, Loss: 2.8140199184417725\n",
      "Epoch 1, Loss: 2.484394073486328\n",
      "Epoch 1, Loss: 3.204134702682495\n",
      "Epoch 1, Loss: 1.7104475498199463\n",
      "Epoch 1, Loss: 3.29890775680542\n",
      "Epoch 1, Loss: 1.9578638076782227\n",
      "Epoch 1, Loss: 2.6256465911865234\n",
      "Epoch 1, Loss: 2.678283452987671\n",
      "Epoch 1, Loss: 1.6785643100738525\n",
      "Epoch 1, Loss: 2.8092799186706543\n",
      "Epoch 1, Loss: 3.3598198890686035\n",
      "Epoch 1, Loss: 1.2796337604522705\n",
      "Epoch 1, Loss: 2.288316011428833\n",
      "Epoch 1, Loss: 2.7776970863342285\n",
      "Epoch 1, Loss: 3.1936585903167725\n",
      "Epoch 1, Loss: 2.922081708908081\n",
      "Epoch 1, Loss: 2.4188036918640137\n",
      "Epoch 1, Loss: 2.9099550247192383\n",
      "Epoch 1, Loss: 3.16013503074646\n",
      "Epoch 1, Loss: 1.8207311630249023\n",
      "Epoch 1, Loss: 2.3908767700195312\n",
      "Epoch 1, Loss: 3.106377601623535\n",
      "Epoch 1, Loss: 2.256319522857666\n",
      "Epoch 1, Loss: 2.4244887828826904\n",
      "Epoch 1, Loss: 3.36501407623291\n",
      "Epoch 1, Loss: 3.003369092941284\n",
      "Epoch 1, Loss: 1.7818996906280518\n",
      "Epoch 1, Loss: 2.3978638648986816\n",
      "Epoch 1, Loss: 2.7823712825775146\n",
      "Epoch 1, Loss: 2.269195318222046\n",
      "Epoch 1, Loss: 1.8285702466964722\n",
      "Epoch 1, Loss: 2.6029117107391357\n",
      "Epoch 1, Loss: 1.9360949993133545\n",
      "Epoch 1, Loss: 3.6492486000061035\n",
      "Epoch 1, Loss: 3.314756393432617\n",
      "Epoch 1, Loss: 2.33184552192688\n",
      "Epoch 1, Loss: 2.156876564025879\n",
      "Epoch 1, Loss: 2.5907602310180664\n",
      "Epoch 1, Loss: 2.7440121173858643\n",
      "Epoch 1, Loss: 2.303903102874756\n",
      "Epoch 1, Loss: 3.1249403953552246\n",
      "Epoch 1, Loss: 3.7399117946624756\n",
      "Epoch 1, Loss: 2.8564789295196533\n",
      "Epoch 1, Loss: 2.7944085597991943\n",
      "Epoch 1, Loss: 2.88240385055542\n",
      "Epoch 1, Loss: 2.518791913986206\n",
      "Epoch 1, Loss: 2.981931209564209\n",
      "Epoch 1, Loss: 2.477968692779541\n",
      "Epoch 1, Loss: 2.508537530899048\n",
      "Epoch 1, Loss: 2.63228702545166\n",
      "Epoch 1, Loss: 3.280670166015625\n",
      "Epoch 1, Loss: 2.8336522579193115\n",
      "Epoch 1, Loss: 2.5803263187408447\n",
      "Epoch 1, Loss: 2.5628552436828613\n",
      "Epoch 1, Loss: 2.44047212600708\n",
      "Epoch 1, Loss: 2.1868808269500732\n",
      "Epoch 1, Loss: 1.3575363159179688\n",
      "Epoch 1, Loss: 2.1684789657592773\n",
      "Epoch 1, Loss: 2.989260196685791\n",
      "Epoch 1, Loss: 2.5123071670532227\n",
      "Epoch 1, Loss: 2.9568095207214355\n",
      "Epoch 1, Loss: 2.0986995697021484\n",
      "Epoch 1, Loss: 2.479525566101074\n",
      "Epoch 1, Loss: 2.167149066925049\n",
      "Epoch 1, Loss: 1.8945050239562988\n",
      "Epoch 1, Loss: 2.820157527923584\n",
      "Epoch 1, Loss: 3.0140461921691895\n",
      "Epoch 1, Loss: 2.4642868041992188\n",
      "Epoch 1, Loss: 2.4928674697875977\n",
      "Epoch 1, Loss: 2.5071563720703125\n",
      "Epoch 1, Loss: 2.186713695526123\n",
      "Epoch 1, Loss: 2.6215665340423584\n",
      "Epoch 1, Loss: 2.3428571224212646\n",
      "Epoch 1, Loss: 3.380064010620117\n",
      "Epoch 1, Loss: 2.559067726135254\n",
      "Epoch 1, Loss: 1.8898756504058838\n",
      "Epoch 1, Loss: 2.5961227416992188\n",
      "Epoch 1, Loss: 3.067723274230957\n",
      "Epoch 1, Loss: 2.2829723358154297\n",
      "Epoch 1, Loss: 3.383615493774414\n",
      "Epoch 1, Loss: 3.4883530139923096\n",
      "Epoch 1, Loss: 2.4754834175109863\n",
      "Epoch 1, Loss: 3.1415839195251465\n",
      "Epoch 1, Loss: 2.4412083625793457\n",
      "Epoch 1, Loss: 3.1916539669036865\n",
      "Epoch 1, Loss: 2.504733085632324\n",
      "Epoch 1, Loss: 2.1028103828430176\n",
      "Epoch 1, Loss: 2.0560224056243896\n",
      "Epoch 1, Loss: 2.494540214538574\n",
      "Epoch 1, Loss: 3.5155105590820312\n",
      "Epoch 1, Loss: 2.453219413757324\n",
      "Epoch 1, Loss: 3.924189567565918\n",
      "Epoch 1, Loss: 2.8436927795410156\n",
      "Epoch 1, Loss: 2.3712971210479736\n",
      "Epoch 1, Loss: 2.469985008239746\n",
      "Epoch 1, Loss: 2.6791458129882812\n",
      "Epoch 1, Loss: 2.9980287551879883\n",
      "Epoch 1, Loss: 2.5117130279541016\n",
      "Epoch 1, Loss: 2.6135621070861816\n",
      "Epoch 1, Loss: 2.9024171829223633\n",
      "Epoch 1, Loss: 2.644719123840332\n",
      "Epoch 1, Loss: 2.4890198707580566\n",
      "Epoch 1, Loss: 2.767082691192627\n",
      "Epoch 1, Loss: 1.6467902660369873\n",
      "Epoch 1, Loss: 2.1971521377563477\n",
      "Epoch 1, Loss: 2.759127616882324\n",
      "Epoch 1, Loss: 4.008818626403809\n",
      "Epoch 1, Loss: 2.1765546798706055\n",
      "Epoch 1, Loss: 3.5181145668029785\n",
      "Epoch 1, Loss: 3.753411054611206\n",
      "Epoch 1, Loss: 2.673290252685547\n",
      "Epoch 1, Loss: 1.9713623523712158\n",
      "Epoch 1, Loss: 2.970025062561035\n",
      "Epoch 1, Loss: 1.6905460357666016\n",
      "Epoch 1, Loss: 2.8722951412200928\n",
      "Epoch 1, Loss: 2.530198097229004\n",
      "Epoch 1, Loss: 2.780853748321533\n",
      "Epoch 1, Loss: 2.2847747802734375\n",
      "Epoch 1, Loss: 2.0327727794647217\n",
      "Epoch 1, Loss: 3.123292922973633\n",
      "Epoch 1, Loss: 1.684678077697754\n",
      "Epoch 1, Loss: 1.6629741191864014\n",
      "Epoch 1, Loss: 1.8977241516113281\n",
      "Epoch 1, Loss: 2.6789536476135254\n",
      "Epoch 1, Loss: 2.1355347633361816\n",
      "Epoch 1, Loss: 3.4671549797058105\n",
      "Epoch 1, Loss: 4.169780731201172\n",
      "Epoch 1, Loss: 2.4085183143615723\n",
      "Epoch 1, Loss: 3.5812292098999023\n",
      "Epoch 1, Loss: 2.2035021781921387\n",
      "Epoch 1, Loss: 2.3618288040161133\n",
      "Epoch 1, Loss: 3.1044228076934814\n",
      "Epoch 1, Loss: 3.0901856422424316\n",
      "Epoch 1, Loss: 2.6520190238952637\n",
      "Epoch 1, Loss: 1.9618148803710938\n",
      "Epoch 1, Loss: 1.8834571838378906\n",
      "Epoch 1, Loss: 1.8615998029708862\n",
      "Epoch 1, Loss: 2.132492780685425\n",
      "Epoch 1, Loss: 3.067789316177368\n",
      "Epoch 1, Loss: 2.6654937267303467\n",
      "Epoch 1, Loss: 2.632765293121338\n",
      "Epoch 1, Loss: 2.6525700092315674\n",
      "Epoch 1, Loss: 3.208563804626465\n",
      "Epoch 1, Loss: 3.6398208141326904\n",
      "Epoch 1, Loss: 2.9962902069091797\n",
      "Epoch 1, Loss: 1.6295912265777588\n",
      "Epoch 1, Loss: 1.7207615375518799\n",
      "Epoch 1, Loss: 2.5289480686187744\n",
      "Epoch 1, Loss: 2.2704386711120605\n",
      "Epoch 1, Loss: 2.2439329624176025\n",
      "Epoch 1, Loss: 2.0324065685272217\n",
      "Epoch 1, Loss: 2.548865795135498\n",
      "Epoch 1, Loss: 2.834501266479492\n",
      "Epoch 1, Loss: 2.092548370361328\n",
      "Epoch 1, Loss: 2.881863594055176\n",
      "Epoch 1, Loss: 2.8018341064453125\n",
      "Epoch 1, Loss: 3.0141196250915527\n",
      "Epoch 1, Loss: 3.1880950927734375\n",
      "Epoch 1, Loss: 2.7050652503967285\n",
      "Epoch 1, Loss: 2.6306097507476807\n",
      "Epoch 1, Loss: 3.025538921356201\n",
      "Epoch 1, Loss: 2.1182708740234375\n",
      "Epoch 1, Loss: 1.9600083827972412\n",
      "Epoch 1, Loss: 1.9126613140106201\n",
      "Epoch 1, Loss: 1.8008008003234863\n",
      "Epoch 1, Loss: 1.8538280725479126\n",
      "Epoch 1, Loss: 2.937167167663574\n",
      "Epoch 1, Loss: 2.174899101257324\n",
      "Epoch 1, Loss: 1.8449335098266602\n",
      "Epoch 1, Loss: 2.081423044204712\n",
      "Epoch 1, Loss: 2.601876974105835\n",
      "Epoch 1, Loss: 2.156644821166992\n",
      "Epoch 1, Loss: 1.9041355848312378\n",
      "Epoch 1, Loss: 2.68568754196167\n",
      "Epoch 1, Loss: 2.588061571121216\n",
      "Epoch 1, Loss: 1.5030467510223389\n",
      "Epoch 1, Loss: 3.3488588333129883\n",
      "Epoch 1, Loss: 3.3326828479766846\n",
      "Epoch 1, Loss: 3.8084912300109863\n",
      "Epoch 1, Loss: 2.065720558166504\n",
      "Epoch 1, Loss: 2.3068108558654785\n",
      "Epoch 1, Loss: 1.4754347801208496\n",
      "Epoch 1, Loss: 2.2093825340270996\n",
      "Epoch 1, Loss: 2.628955364227295\n",
      "Epoch 1, Loss: 2.873466730117798\n",
      "Epoch 1, Loss: 3.091106414794922\n",
      "Epoch 1, Loss: 3.1413774490356445\n",
      "Epoch 1, Loss: 2.8304545879364014\n",
      "Epoch 1, Loss: 2.2864108085632324\n",
      "Epoch 1, Loss: 1.8167166709899902\n",
      "Epoch 1, Loss: 2.503767490386963\n",
      "Epoch 1, Loss: 2.8652186393737793\n",
      "Epoch 1, Loss: 3.359325408935547\n",
      "Epoch 1, Loss: 2.181138277053833\n",
      "Epoch 1, Loss: 2.2354867458343506\n",
      "Epoch 1, Loss: 3.306077241897583\n",
      "Epoch 1, Loss: 2.443166732788086\n",
      "Epoch 1, Loss: 1.6123446226119995\n",
      "Epoch 1, Loss: 2.178992748260498\n",
      "Epoch 1, Loss: 3.12984561920166\n",
      "Epoch 1, Loss: 3.881495475769043\n",
      "Epoch 1, Loss: 1.550459623336792\n",
      "Epoch 1, Loss: 1.771052360534668\n",
      "Epoch 1, Loss: 3.261157512664795\n",
      "Epoch 1, Loss: 1.351212978363037\n",
      "Epoch 1, Loss: 2.2473044395446777\n",
      "Epoch 1, Loss: 2.6602115631103516\n",
      "Epoch 1, Loss: 1.421231985092163\n",
      "Epoch 1, Loss: 3.005599021911621\n",
      "Epoch 1, Loss: 2.4398815631866455\n",
      "Epoch 1, Loss: 2.387362003326416\n",
      "Epoch 1, Loss: 3.9579930305480957\n",
      "Epoch 1, Loss: 1.982588529586792\n",
      "Epoch 1, Loss: 3.661137580871582\n",
      "Epoch 1, Loss: 2.368558406829834\n",
      "Epoch 1, Loss: 2.8906617164611816\n",
      "Epoch 1, Loss: 1.6268553733825684\n",
      "Epoch 1, Loss: 2.2545132637023926\n",
      "Epoch 1, Loss: 2.412038564682007\n",
      "Epoch 1, Loss: 2.1648550033569336\n",
      "Epoch 1, Loss: 1.7314425706863403\n",
      "Epoch 1, Loss: 3.578608989715576\n",
      "Epoch 1, Loss: 2.6445858478546143\n",
      "Epoch 1, Loss: 3.0312952995300293\n",
      "Epoch 1, Loss: 1.4918755292892456\n",
      "Epoch 1, Loss: 2.992249011993408\n",
      "Epoch 1, Loss: 2.432199716567993\n",
      "Epoch 1, Loss: 3.6954307556152344\n",
      "Epoch 1, Loss: 1.7144798040390015\n",
      "Epoch 1, Loss: 2.8212695121765137\n",
      "Epoch 1, Loss: 1.9428651332855225\n",
      "Epoch 1, Loss: 2.000197172164917\n",
      "Epoch 1, Loss: 3.06473708152771\n",
      "Epoch 1, Loss: 2.295004367828369\n",
      "Epoch 1, Loss: 3.005197525024414\n",
      "Epoch 1, Loss: 3.9762911796569824\n",
      "Epoch 1, Loss: 2.3477401733398438\n",
      "Epoch 1, Loss: 2.3776049613952637\n",
      "Epoch 1, Loss: 1.8751647472381592\n",
      "Epoch 1, Loss: 2.6604063510894775\n",
      "Epoch 1, Loss: 2.5363526344299316\n",
      "Epoch 1, Loss: 2.715944766998291\n",
      "Epoch 1, Loss: 2.1548638343811035\n",
      "Epoch 1, Loss: 2.529676675796509\n",
      "Epoch 1, Loss: 2.660911798477173\n",
      "Epoch 1, Loss: 1.416443109512329\n",
      "Epoch 1, Loss: 2.111957550048828\n",
      "Epoch 1, Loss: 2.8323636054992676\n",
      "Epoch 1, Loss: 1.8212414979934692\n",
      "Epoch 1, Loss: 1.3057875633239746\n",
      "Epoch 1, Loss: 2.980452299118042\n",
      "Epoch 1, Loss: 2.6157009601593018\n",
      "Epoch 1, Loss: 3.447728157043457\n",
      "Epoch 1, Loss: 3.381080150604248\n",
      "Epoch 1, Loss: 2.7436087131500244\n",
      "Epoch 1, Loss: 3.1986050605773926\n",
      "Epoch 1, Loss: 1.86424720287323\n",
      "Epoch 1, Loss: 2.2484359741210938\n",
      "Epoch 1, Loss: 2.7154340744018555\n",
      "Epoch 1, Loss: 2.577754020690918\n",
      "Epoch 1, Loss: 3.307551860809326\n",
      "Epoch 1, Loss: 2.8154075145721436\n",
      "Epoch 1, Loss: 3.313600778579712\n",
      "Epoch 1, Loss: 2.6458816528320312\n",
      "Epoch 1, Loss: 2.6501657962799072\n",
      "Epoch 1, Loss: 2.7892308235168457\n",
      "Epoch 1, Loss: 2.7832093238830566\n",
      "Epoch 1, Loss: 2.4305171966552734\n",
      "Epoch 1, Loss: 2.360405445098877\n",
      "Epoch 1, Loss: 1.4480148553848267\n",
      "Epoch 1, Loss: 2.851005792617798\n",
      "Epoch 1, Loss: 3.0703284740448\n",
      "Epoch 1, Loss: 2.7900032997131348\n",
      "Epoch 1, Loss: 2.743680715560913\n",
      "Epoch 1, Loss: 3.4289231300354004\n",
      "Epoch 1, Loss: 2.044269323348999\n",
      "Epoch 1, Loss: 3.9808096885681152\n",
      "Epoch 1, Loss: 2.8001346588134766\n",
      "Epoch 1, Loss: 2.432121992111206\n",
      "Epoch 1, Loss: 2.289292573928833\n",
      "Epoch 1, Loss: 3.655263900756836\n",
      "Epoch 1, Loss: 2.548614025115967\n",
      "Epoch 1, Loss: 4.26218843460083\n",
      "Epoch 1, Loss: 2.6212525367736816\n",
      "Epoch 1, Loss: 2.2454402446746826\n",
      "Epoch 1, Loss: 2.5605111122131348\n",
      "Epoch 1, Loss: 3.1100363731384277\n",
      "Epoch 1, Loss: 2.3000240325927734\n",
      "Epoch 1, Loss: 2.722895622253418\n",
      "Epoch 1, Loss: 2.525151252746582\n",
      "Epoch 1, Loss: 1.8353524208068848\n",
      "Epoch 1, Loss: 2.9634671211242676\n",
      "Epoch 1, Loss: 1.9967221021652222\n",
      "Epoch 1, Loss: 1.6079699993133545\n",
      "Epoch 1, Loss: 2.4381980895996094\n",
      "Epoch 1, Loss: 2.060601234436035\n",
      "Epoch 1, Loss: 4.102838516235352\n",
      "Epoch 1, Loss: 2.4380321502685547\n",
      "Epoch 1, Loss: 4.132109642028809\n",
      "Epoch 1, Loss: 3.366856575012207\n",
      "Epoch 1, Loss: 3.5190534591674805\n",
      "Epoch 1, Loss: 3.116671085357666\n",
      "Epoch 1, Loss: 3.1764931678771973\n",
      "Epoch 1, Loss: 2.0014710426330566\n",
      "Epoch 1, Loss: 2.7295875549316406\n",
      "Epoch 1, Loss: 1.7902171611785889\n",
      "Epoch 1, Loss: 1.5797247886657715\n",
      "Epoch 1, Loss: 2.278017044067383\n",
      "Epoch 1, Loss: 2.0029075145721436\n",
      "Epoch 1, Loss: 1.7706314325332642\n",
      "Epoch 1, Loss: 1.8548526763916016\n",
      "Epoch 1, Loss: 1.7105870246887207\n",
      "Epoch 1, Loss: 3.33083438873291\n",
      "Epoch 1, Loss: 2.606783866882324\n",
      "Epoch 1, Loss: 3.839171886444092\n",
      "Epoch 1, Loss: 1.4644289016723633\n",
      "Epoch 1, Loss: 3.425048351287842\n",
      "Epoch 1, Loss: 3.280414581298828\n",
      "Epoch 1, Loss: 3.376744270324707\n",
      "Epoch 1, Loss: 1.5514267683029175\n",
      "Epoch 1, Loss: 2.419358491897583\n",
      "Epoch 1, Loss: 2.8287200927734375\n",
      "Epoch 1, Loss: 1.8379271030426025\n",
      "Epoch 1, Loss: 3.0438499450683594\n",
      "Epoch 1, Loss: 2.776141881942749\n",
      "Epoch 1, Loss: 2.522552490234375\n",
      "Epoch 1, Loss: 3.2360897064208984\n",
      "Epoch 1, Loss: 2.990039110183716\n",
      "Epoch 1, Loss: 2.4851231575012207\n",
      "Epoch 1, Loss: 2.145512104034424\n",
      "Epoch 1, Loss: 4.015044212341309\n",
      "Epoch 1, Loss: 2.057636022567749\n",
      "Epoch 1, Loss: 2.03783917427063\n",
      "Epoch 1, Loss: 1.904010534286499\n",
      "Epoch 1, Loss: 2.7236523628234863\n",
      "Epoch 1, Loss: 2.7979087829589844\n",
      "Epoch 1, Loss: 2.187838077545166\n",
      "Epoch 1, Loss: 2.407264232635498\n",
      "Epoch 1, Loss: 2.7825727462768555\n",
      "Epoch 1, Loss: 2.903482437133789\n",
      "Epoch 1, Loss: 3.1653809547424316\n",
      "Epoch 1, Loss: 2.467979907989502\n",
      "Epoch 1, Loss: 2.2322275638580322\n",
      "Epoch 1, Loss: 1.8523681163787842\n",
      "Epoch 1, Loss: 2.505091428756714\n",
      "Epoch 1, Loss: 2.6691341400146484\n",
      "Epoch 1, Loss: 1.680359125137329\n",
      "Epoch 1, Loss: 2.1937496662139893\n",
      "Epoch 1, Loss: 2.1147944927215576\n",
      "Epoch 1, Loss: 1.1822595596313477\n",
      "Epoch 1, Loss: 2.8276405334472656\n",
      "Epoch 1, Loss: 1.866158366203308\n",
      "Epoch 1, Loss: 2.322382688522339\n",
      "Epoch 1, Loss: 3.710509777069092\n",
      "Epoch 1, Loss: 3.016418218612671\n",
      "Epoch 1, Loss: 2.08388614654541\n",
      "Epoch 1, Loss: 1.9586358070373535\n",
      "Epoch 1, Loss: 2.0429234504699707\n",
      "Epoch 1, Loss: 2.494554042816162\n",
      "Epoch 1, Loss: 2.7660458087921143\n",
      "Epoch 1, Loss: 2.67021107673645\n",
      "Epoch 1, Loss: 2.5877275466918945\n",
      "Epoch 1, Loss: 2.9028255939483643\n",
      "Epoch 1, Loss: 2.106252670288086\n",
      "Epoch 1, Loss: 1.899402379989624\n",
      "Epoch 1, Loss: 2.6753973960876465\n",
      "Epoch 1, Loss: 2.854746103286743\n",
      "Epoch 1, Loss: 2.9959867000579834\n",
      "Epoch 1, Loss: 2.2903084754943848\n",
      "Epoch 1, Loss: 2.489534854888916\n",
      "Epoch 1, Loss: 3.4122133255004883\n",
      "Epoch 1, Loss: 2.5139570236206055\n",
      "Epoch 1, Loss: 2.5871760845184326\n",
      "Epoch 1, Loss: 2.7825708389282227\n",
      "Epoch 1, Loss: 2.4533114433288574\n",
      "Epoch 1, Loss: 2.583477020263672\n",
      "Epoch 1, Loss: 3.6125965118408203\n",
      "Epoch 1, Loss: 2.2576026916503906\n",
      "Epoch 1, Loss: 2.171762704849243\n",
      "Epoch 1, Loss: 3.0084292888641357\n",
      "Epoch 1, Loss: 2.3783740997314453\n",
      "Epoch 1, Loss: 2.3889853954315186\n",
      "Epoch 1, Loss: 2.9620542526245117\n",
      "Epoch 1, Loss: 1.7356493473052979\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "# Load your model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assume train_loader is defined somewhere else, as provided in previous responses\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Ensure this matches your dataset setup\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  # Adjust learning rate as needed\n",
    "\n",
    "# Compute the total number of training steps based on your data loader and desired number of epochs\n",
    "num_epochs = 2  # Adjust number of epochs here\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Setup the learning rate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=num_training_steps // 10,  # Usually 10% of train steps\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Sample training loop skeleton (for completeness)\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs = {\n",
    "            'input_ids': batch[0].to(model.device), \n",
    "            'attention_mask': batch[1].to(model.device),\n",
    "            'start_positions': batch[3].to(model.device),\n",
    "            'end_positions': batch[4].to(model.device)\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        lr_scheduler.step()  # Update learning rate\n",
    "        optimizer.zero_grad()  # Clear gradients for next batch\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")  # Output loss for tracking progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3191246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bbed5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09baef71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867df4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd97141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c8848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a8c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ea7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eadc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb15eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ae72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17ca6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e234c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11ed17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ba048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a0c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844a0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f739af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b08c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa2211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee4203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970cf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1acb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c276a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b52c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e135312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb18a846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cf747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ef433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beea1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "def convert_to_tensors(features):\n",
    "    # Convert lists of examples into tensors for each feature\n",
    "    all_input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f['token_type_ids'] for f in features], dtype=torch.long)\n",
    "    all_start_positions = torch.tensor([f['start_positions'] for f in features], dtype=torch.long)\n",
    "    all_end_positions = torch.tensor([f['end_positions'] for f in features], dtype=torch.long)\n",
    "\n",
    "    return TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_start_positions, all_end_positions)\n",
    "\n",
    "# Assuming you have a function or method that produces tokenized_datasets\n",
    "# For example:\n",
    "# train_features = tokenize_squad_examples(train_examples)\n",
    "# validation_features = tokenize_squad_examples(validation_examples)\n",
    "\n",
    "train_dataset = convert_to_tensors(tokenized_datasets['train'])\n",
    "validation_dataset = convert_to_tensors(tokenized_datasets['validation'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff8bfd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the optimizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)  \u001b[38;5;66;03m# Adjust learning rate as needed\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the total number of training steps\u001b[39;00m\n\u001b[1;32m      5\u001b[0m num_training_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Assuming 3 epochs, adjust as needed\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  # Adjust learning rate as needed\n",
    "\n",
    "# Compute the total number of training steps\n",
    "num_training_steps = len(train_loader) * 2  # Assuming 3 epochs, adjust as needed\n",
    "\n",
    "# Setup the learning rate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=num_training_steps // 10,  # Usually 10% of train steps\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f657c4f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Adjust the number of epochs if needed\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(1):  # Adjust the number of epochs if needed\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, token_type_ids, start_positions, end_positions = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        start_positions=start_positions, \n",
    "                        end_positions=end_positions)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} average training loss: {avg_train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc038b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
